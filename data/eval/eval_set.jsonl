{"question": "关于prompt-based、one-shot、exact的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs", "expected_source": "data\\arxiv\\arxiv_2508.13805v1.md"}
{"question": "围绕prompt-based、one-shot、exact，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs", "expected_source": "data\\arxiv\\arxiv_2508.13805v1.md"}
{"question": "关于is-nerf、in-scattering、neural的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images", "expected_source": "data\\arxiv\\arxiv_2508.13808v1.md"}
{"question": "围绕is-nerf、in-scattering、neural，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images", "expected_source": "data\\arxiv\\arxiv_2508.13808v1.md"}
{"question": "关于illusion、perfect、metric的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "The illusion of a perfect metric: Why evaluating AI's words is harder   than it looks", "expected_source": "data\\arxiv\\arxiv_2508.13816v1.md"}
{"question": "围绕illusion、perfect、metric，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "The illusion of a perfect metric: Why evaluating AI's words is harder   than it looks", "expected_source": "data\\arxiv\\arxiv_2508.13816v1.md"}
{"question": "关于latent、interpolation、learning的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction", "expected_source": "data\\arxiv\\arxiv_2508.13826v2.md"}
{"question": "围绕latent、interpolation、learning，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Latent Interpolation Learning Using Diffusion Models for Cardiac Volume   Reconstruction", "expected_source": "data\\arxiv\\arxiv_2508.13826v2.md"}
{"question": "关于saga、learning、signal-aligned的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image   Generation", "expected_source": "data\\arxiv\\arxiv_2508.13866v1.md"}
{"question": "围绕saga、learning、signal-aligned，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image   Generation", "expected_source": "data\\arxiv\\arxiv_2508.13866v1.md"}
{"question": "关于improved、generalized、planning的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Improved Generalized Planning with LLMs through Strategy Refinement and   Reflection", "expected_source": "data\\arxiv\\arxiv_2508.13876v1.md"}
{"question": "围绕improved、generalized、planning，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Improved Generalized Planning with LLMs through Strategy Refinement and   Reflection", "expected_source": "data\\arxiv\\arxiv_2508.13876v1.md"}
{"question": "关于revisiting、diffusion、q-learning的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step   Action Generation", "expected_source": "data\\arxiv\\arxiv_2508.13904v1.md"}
{"question": "围绕revisiting、diffusion、q-learning，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step   Action Generation", "expected_source": "data\\arxiv\\arxiv_2508.13904v1.md"}
{"question": "关于structured、agentic、workflows的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Structured Agentic Workflows for Financial Time-Series Modeling with   LLMs and Reflective Feedback", "expected_source": "data\\arxiv\\arxiv_2508.13915v1.md"}
{"question": "围绕structured、agentic、workflows，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Structured Agentic Workflows for Financial Time-Series Modeling with   LLMs and Reflective Feedback", "expected_source": "data\\arxiv\\arxiv_2508.13915v1.md"}
{"question": "关于inpars、supercharging、synthetic的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "InPars+: Supercharging Synthetic Data Generation for Information   Retrieval Systems", "expected_source": "data\\arxiv\\arxiv_2508.13930v1.md"}
{"question": "围绕inpars、supercharging、synthetic，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "InPars+: Supercharging Synthetic Data Generation for Information   Retrieval Systems", "expected_source": "data\\arxiv\\arxiv_2508.13930v1.md"}
{"question": "关于collaboration、paradox、why的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic   Intelligence and Operational Stability in Supply Chain Management", "expected_source": "data\\arxiv\\arxiv_2508.13942v1.md"}
{"question": "围绕collaboration、paradox、why，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic   Intelligence and Operational Stability in Supply Chain Management", "expected_source": "data\\arxiv\\arxiv_2508.13942v1.md"}
{"question": "关于prompt、orchestration、markup的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Prompt Orchestration Markup Language", "expected_source": "data\\arxiv\\arxiv_2508.13948v1.md"}
{"question": "围绕prompt、orchestration、markup，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Prompt Orchestration Markup Language", "expected_source": "data\\arxiv\\arxiv_2508.13948v1.md"}
{"question": "关于learning、how、can的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Learning to Use AI for Learning: How Can We Effectively Teach and   Measure Prompting Literacy for K-12 Students?", "expected_source": "data\\arxiv\\arxiv_2508.13962v1.md"}
{"question": "围绕learning、how、can，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Learning to Use AI for Learning: How Can We Effectively Teach and   Measure Prompting Literacy for K-12 Students?", "expected_source": "data\\arxiv\\arxiv_2508.13962v1.md"}
{"question": "关于chronollm、customizing、physics-based的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code   Generation", "expected_source": "data\\arxiv\\arxiv_2508.13975v1.md"}
{"question": "围绕chronollm、customizing、physics-based，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code   Generation", "expected_source": "data\\arxiv\\arxiv_2508.13975v1.md"}
{"question": "关于chunks、arms、multi-armed的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization", "expected_source": "data\\arxiv\\arxiv_2508.13993v1.md"}
{"question": "围绕chunks、arms、multi-armed，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM   Preference Optimization", "expected_source": "data\\arxiv\\arxiv_2508.13993v1.md"}
{"question": "关于ask、good、questions的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Ask Good Questions for Large Language Models", "expected_source": "data\\arxiv\\arxiv_2508.14025v1.md"}
{"question": "围绕ask、good、questions，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Ask Good Questions for Large Language Models", "expected_source": "data\\arxiv\\arxiv_2508.14025v1.md"}
{"question": "关于unintended、misalignment、agentic的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "expected_source": "data\\arxiv\\arxiv_2508.14031v1.md"}
{"question": "围绕unintended、misalignment、agentic，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation", "expected_source": "data\\arxiv\\arxiv_2508.14031v1.md"}
{"question": "关于dpad、efficient、diffusion的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "expected_source": "data\\arxiv\\arxiv_2508.14148v1.md"}
{"question": "围绕dpad、efficient、diffusion，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "expected_source": "data\\arxiv\\arxiv_2508.14148v1.md"}
{"question": "关于two、birds、one的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Two Birds with One Stone: Multi-Task Detection and Attribution of   LLM-Generated Text", "expected_source": "data\\arxiv\\arxiv_2508.14190v1.md"}
{"question": "围绕two、birds、one，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Two Birds with One Stone: Multi-Task Detection and Attribution of   LLM-Generated Text", "expected_source": "data\\arxiv\\arxiv_2508.14190v1.md"}
{"question": "关于highly、aligned、human的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Large Language Models are Highly Aligned with Human Ratings of Emotional   Stimuli", "expected_source": "data\\arxiv\\arxiv_2508.14214v1.md"}
{"question": "围绕highly、aligned、human，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Large Language Models are Highly Aligned with Human Ratings of Emotional   Stimuli", "expected_source": "data\\arxiv\\arxiv_2508.14214v1.md"}
{"question": "关于disentangling、concept、semantics的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Disentangling concept semantics via multilingual averaging in Sparse   Autoencoders", "expected_source": "data\\arxiv\\arxiv_2508.14275v1.md"}
{"question": "围绕disentangling、concept、semantics，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Disentangling concept semantics via multilingual averaging in Sparse   Autoencoders", "expected_source": "data\\arxiv\\arxiv_2508.14275v1.md"}
{"question": "关于tooth-diffusion、guided、cbct的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth   Conditioning", "expected_source": "data\\arxiv\\arxiv_2508.14276v1.md"}
{"question": "围绕tooth-diffusion、guided、cbct，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth   Conditioning", "expected_source": "data\\arxiv\\arxiv_2508.14276v1.md"}
{"question": "关于amortized、bayesian、meta-learning的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large   Language Models", "expected_source": "data\\arxiv\\arxiv_2508.14285v1.md"}
{"question": "围绕amortized、bayesian、meta-learning，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large   Language Models", "expected_source": "data\\arxiv\\arxiv_2508.14285v1.md"}
{"question": "关于explaining、hitori、puzzles的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential   Decisions", "expected_source": "data\\arxiv\\arxiv_2508.14294v1.md"}
{"question": "围绕explaining、hitori、puzzles，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential   Decisions", "expected_source": "data\\arxiv\\arxiv_2508.14294v1.md"}
{"question": "关于glass、test-time、acceleration的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural   Importance Aggregation", "expected_source": "data\\arxiv\\arxiv_2508.14302v1.md"}
{"question": "围绕glass、test-time、acceleration，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural   Importance Aggregation", "expected_source": "data\\arxiv\\arxiv_2508.14302v1.md"}
{"question": "关于your、reward、function的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS", "expected_source": "data\\arxiv\\arxiv_2508.14313v1.md"}
{"question": "围绕your、reward、function，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS", "expected_source": "data\\arxiv\\arxiv_2508.14313v1.md"}
{"question": "关于zero-knowledge、llm、hallucination的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Zero-knowledge LLM hallucination detection and mitigation through   fine-grained cross-model consistency", "expected_source": "data\\arxiv\\arxiv_2508.14314v1.md"}
{"question": "围绕zero-knowledge、llm、hallucination，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Zero-knowledge LLM hallucination detection and mitigation through   fine-grained cross-model consistency", "expected_source": "data\\arxiv\\arxiv_2508.14314v1.md"}
{"question": "关于moviedrive、multi-modal、multi-view的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation", "expected_source": "data\\arxiv\\arxiv_2508.14327v1.md"}
{"question": "围绕moviedrive、multi-modal、multi-view，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation", "expected_source": "data\\arxiv\\arxiv_2508.14327v1.md"}
{"question": "关于generative、against、poaching的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Generative AI Against Poaching: Latent Composite Flow Matching for   Wildlife Conservation", "expected_source": "data\\arxiv\\arxiv_2508.14342v1.md"}
{"question": "围绕generative、against、poaching，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Generative AI Against Poaching: Latent Composite Flow Matching for   Wildlife Conservation", "expected_source": "data\\arxiv\\arxiv_2508.14342v1.md"}
{"question": "关于non-asymptotic、convergent、analysis的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative   Model via a System of Stochastic Differential Equations", "expected_source": "data\\arxiv\\arxiv_2508.14351v1.md"}
{"question": "围绕non-asymptotic、convergent、analysis，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative   Model via a System of Stochastic Differential Equations", "expected_source": "data\\arxiv\\arxiv_2508.14351v1.md"}
{"question": "关于sbgd、improving、graph的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "SBGD: Improving Graph Diffusion Generative Model via Stochastic Block   Diffusion", "expected_source": "data\\arxiv\\arxiv_2508.14352v1.md"}
{"question": "围绕sbgd、improving、graph，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "SBGD: Improving Graph Diffusion Generative Model via Stochastic Block   Diffusion", "expected_source": "data\\arxiv\\arxiv_2508.14352v1.md"}
{"question": "关于organ-agents、virtual、human的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Organ-Agents: Virtual Human Physiology Simulator via LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14357v1.md"}
{"question": "围绕organ-agents、virtual、human，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Organ-Agents: Virtual Human Physiology Simulator via LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14357v1.md"}
{"question": "关于physics-constrained、diffusion、reconstruction的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Physics-Constrained Diffusion Reconstruction with Posterior Correction   for Quantitative and Fast PET Imaging", "expected_source": "data\\arxiv\\arxiv_2508.14364v1.md"}
{"question": "围绕physics-constrained、diffusion、reconstruction，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Physics-Constrained Diffusion Reconstruction with Posterior Correction   for Quantitative and Fast PET Imaging", "expected_source": "data\\arxiv\\arxiv_2508.14364v1.md"}
{"question": "关于zpd-sca、unveiling、blind的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students'   Cognitive Abilities", "expected_source": "data\\arxiv\\arxiv_2508.14377v1.md"}
{"question": "围绕zpd-sca、unveiling、blind，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students'   Cognitive Abilities", "expected_source": "data\\arxiv\\arxiv_2508.14377v1.md"}
{"question": "关于credence、calibration、game的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Credence Calibration Game? Calibrating Large Language Models through   Structured Play", "expected_source": "data\\arxiv\\arxiv_2508.14390v1.md"}
{"question": "围绕credence、calibration、game，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Credence Calibration Game? Calibrating Large Language Models through   Structured Play", "expected_source": "data\\arxiv\\arxiv_2508.14390v1.md"}
{"question": "关于depth、hallucination-free、relation的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware   Sentence Simplification and Two-tiered Hierarchical Refinement", "expected_source": "data\\arxiv\\arxiv_2508.14391v1.md"}
{"question": "围绕depth、hallucination-free、relation，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware   Sentence Simplification and Two-tiered Hierarchical Refinement", "expected_source": "data\\arxiv\\arxiv_2508.14391v1.md"}
{"question": "关于cta-flux、integrating、chinese的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality   English Text-to-Image Communities", "expected_source": "data\\arxiv\\arxiv_2508.14405v1.md"}
{"question": "围绕cta-flux、integrating、chinese，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality   English Text-to-Image Communities", "expected_source": "data\\arxiv\\arxiv_2508.14405v1.md"}
{"question": "关于cognitive、surgery、awakening的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in   LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14408v1.md"}
{"question": "围绕cognitive、surgery、awakening，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in   LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14408v1.md"}
{"question": "关于automated、optimization、modeling的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning", "expected_source": "data\\arxiv\\arxiv_2508.14410v1.md"}
{"question": "围绕automated、optimization、modeling，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning", "expected_source": "data\\arxiv\\arxiv_2508.14410v1.md"}
{"question": "关于real-world、display、inverse的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "A Real-world Display Inverse Rendering Dataset", "expected_source": "data\\arxiv\\arxiv_2508.14411v1.md"}
{"question": "围绕real-world、display、inverse，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "A Real-world Display Inverse Rendering Dataset", "expected_source": "data\\arxiv\\arxiv_2508.14411v1.md"}
{"question": "关于disentanglement、t-space、faster的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Disentanglement in T-space for Faster and Distributed Training of   Diffusion Models with Fewer Latent-states", "expected_source": "data\\arxiv\\arxiv_2508.14413v1.md"}
{"question": "围绕disentanglement、t-space、faster，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Disentanglement in T-space for Faster and Distributed Training of   Diffusion Models with Fewer Latent-states", "expected_source": "data\\arxiv\\arxiv_2508.14413v1.md"}
{"question": "关于hyperdiff、hypergraph、guided的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose   Estimation", "expected_source": "data\\arxiv\\arxiv_2508.14431v1.md"}
{"question": "围绕hyperdiff、hypergraph、guided，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose   Estimation", "expected_source": "data\\arxiv\\arxiv_2508.14431v1.md"}
{"question": "关于focus、frequency-optimized、conditioning的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for   mitigating catastrophic forgetting during Test-Time Adaptation", "expected_source": "data\\arxiv\\arxiv_2508.14437v1.md"}
{"question": "围绕focus、frequency-optimized、conditioning，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for   mitigating catastrophic forgetting during Test-Time Adaptation", "expected_source": "data\\arxiv\\arxiv_2508.14437v1.md"}
{"question": "关于muse、multi-subject、unified的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic   Expansion", "expected_source": "data\\arxiv\\arxiv_2508.14440v1.md"}
{"question": "围绕muse、multi-subject、unified，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic   Expansion", "expected_source": "data\\arxiv\\arxiv_2508.14440v1.md"}
{"question": "关于dupo、enabling、reliable的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference   Optimization", "expected_source": "data\\arxiv\\arxiv_2508.14460v1.md"}
{"question": "围绕dupo、enabling、reliable，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference   Optimization", "expected_source": "data\\arxiv\\arxiv_2508.14460v1.md"}
{"question": "关于ouroboros、single-step、diffusion的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and   Inverse Rendering", "expected_source": "data\\arxiv\\arxiv_2508.14461v1.md"}
{"question": "围绕ouroboros、single-step、diffusion，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and   Inverse Rendering", "expected_source": "data\\arxiv\\arxiv_2508.14461v1.md"}
{"question": "关于in2x、wmt25、translation的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "In2x at WMT25 Translation Task", "expected_source": "data\\arxiv\\arxiv_2508.14472v1.md"}
{"question": "围绕in2x、wmt25、translation，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "In2x at WMT25 Translation Task", "expected_source": "data\\arxiv\\arxiv_2508.14472v1.md"}
{"question": "关于vivid-vr、distilling、concepts的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer   for Photorealistic Video Restoration", "expected_source": "data\\arxiv\\arxiv_2508.14483v1.md"}
{"question": "围绕vivid-vr、distilling、concepts，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer   for Photorealistic Video Restoration", "expected_source": "data\\arxiv\\arxiv_2508.14483v1.md"}
{"question": "关于semantic、energy、detecting的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy", "expected_source": "data\\arxiv\\arxiv_2508.14496v1.md"}
{"question": "围绕semantic、energy、detecting，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Semantic Energy: Detecting LLM Hallucination Beyond Entropy", "expected_source": "data\\arxiv\\arxiv_2508.14496v1.md"}
{"question": "关于saturn、autoregressive、image的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "SATURN: Autoregressive Image Generation Guided by Scene Graphs", "expected_source": "data\\arxiv\\arxiv_2508.14502v1.md"}
{"question": "围绕saturn、autoregressive、image，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "SATURN: Autoregressive Image Generation Guided by Scene Graphs", "expected_source": "data\\arxiv\\arxiv_2508.14502v1.md"}
{"question": "关于wise-fuse、efficient、whole的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch   Selection with VLM and LLM Knowledge Fusion", "expected_source": "data\\arxiv\\arxiv_2508.14537v1.md"}
{"question": "围绕wise-fuse、efficient、whole，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch   Selection with VLM and LLM Knowledge Fusion", "expected_source": "data\\arxiv\\arxiv_2508.14537v1.md"}
{"question": "关于post-hoc、llm-supported、debugging的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Post-hoc LLM-Supported Debugging of Distributed Processes", "expected_source": "data\\arxiv\\arxiv_2508.14540v1.md"}
{"question": "围绕post-hoc、llm-supported、debugging，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Post-hoc LLM-Supported Debugging of Distributed Processes", "expected_source": "data\\arxiv\\arxiv_2508.14540v1.md"}
{"question": "关于adaptively、robust、llm的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Adaptively Robust LLM Inference Optimization under Prediction   Uncertainty", "expected_source": "data\\arxiv\\arxiv_2508.14544v1.md"}
{"question": "围绕adaptively、robust、llm，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Adaptively Robust LLM Inference Optimization under Prediction   Uncertainty", "expected_source": "data\\arxiv\\arxiv_2508.14544v1.md"}
{"question": "关于llm-generated、explanations、component-based的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Towards LLM-generated explanations for Component-based Knowledge Graph   Question Answering Systems", "expected_source": "data\\arxiv\\arxiv_2508.14553v1.md"}
{"question": "围绕llm-generated、explanations、component-based，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Towards LLM-generated explanations for Component-based Knowledge Graph   Question Answering Systems", "expected_source": "data\\arxiv\\arxiv_2508.14553v1.md"}
{"question": "关于gogs、high-fidelity、geometry的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via   Gaussian Surfels", "expected_source": "data\\arxiv\\arxiv_2508.14563v1.md"}
{"question": "围绕gogs、high-fidelity、geometry，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via   Gaussian Surfels", "expected_source": "data\\arxiv\\arxiv_2508.14563v1.md"}
{"question": "关于who、sees、what的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Who Sees What? Structured Thought-Action Sequences for Epistemic   Reasoning in LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14564v1.md"}
{"question": "围绕who、sees、what，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Who Sees What? Structured Thought-Action Sequences for Epistemic   Reasoning in LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14564v1.md"}
{"question": "关于anchorsync、global、consistency的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "AnchorSync: Global Consistency Optimization for Long Video Editing", "expected_source": "data\\arxiv\\arxiv_2508.14609v1.md"}
{"question": "围绕anchorsync、global、consistency，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "AnchorSync: Global Consistency Optimization for Long Video Editing", "expected_source": "data\\arxiv\\arxiv_2508.14609v1.md"}
{"question": "关于can、llm、agents的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware   Planning and Coordination", "expected_source": "data\\arxiv\\arxiv_2508.14635v1.md"}
{"question": "围绕can、llm、agents，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware   Planning and Coordination", "expected_source": "data\\arxiv\\arxiv_2508.14635v1.md"}
{"question": "关于entropy-constrained、strategy、optimization的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent   Framework with LLM and Knowledge Graph Integration", "expected_source": "data\\arxiv\\arxiv_2508.14654v1.md"}
{"question": "围绕entropy-constrained、strategy、optimization，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent   Framework with LLM and Knowledge Graph Integration", "expected_source": "data\\arxiv\\arxiv_2508.14654v1.md"}
{"question": "关于virtual、multiplex、staining的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Virtual Multiplex Staining for Histological Images using a Marker-wise   Conditioned Diffusion Model", "expected_source": "data\\arxiv\\arxiv_2508.14681v1.md"}
{"question": "围绕virtual、multiplex、staining，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Virtual Multiplex Staining for Histological Images using a Marker-wise   Conditioned Diffusion Model", "expected_source": "data\\arxiv\\arxiv_2508.14681v1.md"}
{"question": "关于mcp-universe、benchmarking、real-world的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model   Context Protocol Servers", "expected_source": "data\\arxiv\\arxiv_2508.14704v1.md"}
{"question": "围绕mcp-universe、benchmarking、real-world，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model   Context Protocol Servers", "expected_source": "data\\arxiv\\arxiv_2508.14704v1.md"}
{"question": "关于shizhengpt、multimodal、llms的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "expected_source": "data\\arxiv\\arxiv_2508.14706v1.md"}
{"question": "围绕shizhengpt、multimodal、llms，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "expected_source": "data\\arxiv\\arxiv_2508.14706v1.md"}
{"question": "关于gsfix3d、diffusion-guided、repair的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting", "expected_source": "data\\arxiv\\arxiv_2508.14717v1.md"}
{"question": "围绕gsfix3d、diffusion-guided、repair，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting", "expected_source": "data\\arxiv\\arxiv_2508.14717v1.md"}
{"question": "关于transplant、then、regenerate的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "expected_source": "data\\arxiv\\arxiv_2508.14723v1.md"}
{"question": "围绕transplant、then、regenerate，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "expected_source": "data\\arxiv\\arxiv_2508.14723v1.md"}
{"question": "关于assessing、quality、security的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative   Analysis", "expected_source": "data\\arxiv\\arxiv_2508.14727v1.md"}
{"question": "围绕assessing、quality、security，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative   Analysis", "expected_source": "data\\arxiv\\arxiv_2508.14727v1.md"}
{"question": "关于evaluating、multilingual、code-switched的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference", "expected_source": "data\\arxiv\\arxiv_2508.14735v1.md"}
{"question": "围绕evaluating、multilingual、code-switched，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference", "expected_source": "data\\arxiv\\arxiv_2508.14735v1.md"}
{"question": "关于missionhd、data-driven、refinement的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through   Hyperdimensional Causal Path Encoding and Decoding", "expected_source": "data\\arxiv\\arxiv_2508.14746v1.md"}
{"question": "围绕missionhd、data-driven、refinement，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "MissionHD: Data-Driven Refinement of Reasoning Graph Structure through   Hyperdimensional Causal Path Encoding and Decoding", "expected_source": "data\\arxiv\\arxiv_2508.14746v1.md"}
{"question": "关于cross-modality、controlled、molecule的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Cross-Modality Controlled Molecule Generation with Diffusion Language   Model", "expected_source": "data\\arxiv\\arxiv_2508.14748v1.md"}
{"question": "围绕cross-modality、controlled、molecule，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Cross-Modality Controlled Molecule Generation with Diffusion Language   Model", "expected_source": "data\\arxiv\\arxiv_2508.14748v1.md"}
{"question": "关于herakles、hierarchical、skill的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents", "expected_source": "data\\arxiv\\arxiv_2508.14751v1.md"}
{"question": "围绕herakles、hierarchical、skill，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents", "expected_source": "data\\arxiv\\arxiv_2508.14751v1.md"}
{"question": "关于reliable、generation、isomorphic的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Reliable generation of isomorphic physics problems using ChatGPT with   prompt-chaining and tool use", "expected_source": "data\\arxiv\\arxiv_2508.14755v1.md"}
{"question": "围绕reliable、generation、isomorphic，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Reliable generation of isomorphic physics problems using ChatGPT with   prompt-chaining and tool use", "expected_source": "data\\arxiv\\arxiv_2508.14755v1.md"}
{"question": "关于pepthink-r1、llm、interpretable的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT   SFT and Reinforcement Learning", "expected_source": "data\\arxiv\\arxiv_2508.14765v1.md"}
{"question": "围绕pepthink-r1、llm、interpretable，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT   SFT and Reinforcement Learning", "expected_source": "data\\arxiv\\arxiv_2508.14765v1.md"}
{"question": "关于transllm、unified、multi-task的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting", "expected_source": "data\\arxiv\\arxiv_2508.14782v1.md"}
{"question": "围绕transllm、unified、multi-task，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting", "expected_source": "data\\arxiv\\arxiv_2508.14782v1.md"}
{"question": "关于privileged、self-access、matters的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Privileged Self-Access Matters for Introspection in AI", "expected_source": "data\\arxiv\\arxiv_2508.14802v1.md"}
{"question": "围绕privileged、self-access、matters，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Privileged Self-Access Matters for Introspection in AI", "expected_source": "data\\arxiv\\arxiv_2508.14802v1.md"}
{"question": "关于tinker、diffusion、gift的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From   Sparse Inputs without Per-Scene Optimization", "expected_source": "data\\arxiv\\arxiv_2508.14811v1.md"}
{"question": "围绕tinker、diffusion、gift，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From   Sparse Inputs without Per-Scene Optimization", "expected_source": "data\\arxiv\\arxiv_2508.14811v1.md"}
{"question": "关于translight、image-guided、customized的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "TransLight: Image-Guided Customized Lighting Control with Generative   Decoupling", "expected_source": "data\\arxiv\\arxiv_2508.14814v1.md"}
{"question": "围绕translight、image-guided、customized，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "TransLight: Image-Guided Customized Lighting Control with Generative   Decoupling", "expected_source": "data\\arxiv\\arxiv_2508.14814v1.md"}
{"question": "关于evaluating、retrieval-augmented、generation的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs", "expected_source": "data\\arxiv\\arxiv_2508.14817v1.md"}
{"question": "围绕evaluating、retrieval-augmented、generation，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs", "expected_source": "data\\arxiv\\arxiv_2508.14817v1.md"}
{"question": "关于long、chain-of-thought、reasoning的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Long Chain-of-Thought Reasoning Across Languages", "expected_source": "data\\arxiv\\arxiv_2508.14828v1.md"}
{"question": "围绕long、chain-of-thought、reasoning，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Long Chain-of-Thought Reasoning Across Languages", "expected_source": "data\\arxiv\\arxiv_2508.14828v1.md"}
{"question": "关于universal、transferable、adversarial的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent", "expected_source": "data\\arxiv\\arxiv_2508.14853v1.md"}
{"question": "围绕universal、transferable、adversarial，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent", "expected_source": "data\\arxiv\\arxiv_2508.14853v1.md"}
{"question": "关于squeezed、diffusion、typically的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Squeezed Diffusion Models", "expected_source": "data\\arxiv\\arxiv_2508.14871v1.md"}
{"question": "围绕squeezed、diffusion、typically，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Squeezed Diffusion Models", "expected_source": "data\\arxiv\\arxiv_2508.14871v1.md"}
{"question": "关于meshcoder、llm-powered、structured的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds", "expected_source": "data\\arxiv\\arxiv_2508.14879v1.md"}
{"question": "围绕meshcoder、llm-powered、structured，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds", "expected_source": "data\\arxiv\\arxiv_2508.14879v1.md"}
{"question": "关于quantization、meets、dllms的论文，其核心贡献是什么？请给出要点并附来源编号。", "expected_title": "Quantization Meets dLLMs: A Systematic Study of Post-training   Quantization for Diffusion LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14896v1.md"}
{"question": "围绕quantization、meets、dllms，与以往方法相比的关键差异是什么？请附来源编号。", "expected_title": "Quantization Meets dLLMs: A Systematic Study of Post-training   Quantization for Diffusion LLMs", "expected_source": "data\\arxiv\\arxiv_2508.14896v1.md"}
